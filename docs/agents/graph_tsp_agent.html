<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>agents.graph_tsp_agent API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>agents.graph_tsp_agent</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import csv
import logging
import os
import time
from copy import deepcopy
from typing import Tuple

import numpy as np
import torch
import torch.nn as nn
from scipy import stats

from .graph_decoder import GraphDecoder
from .graph_encoder import GraphEncoder

logging.basicConfig(level=logging.INFO)


class TSPModel(nn.Module):
    def __init__(
        self,
        node_dim: int,
        emb_dim: int,
        hidden_dim: int,
        num_attention_layers: int,
        num_heads: int,
    ):
        &#34;&#34;&#34;
        The TSPModel is used in companionship with the TSPEnv
        to solve the capacited vehicle routing problem.

        Args:
            depot_dim (int): Input dimension of a depot node.
            node_dim (int): Input dimension of a regular graph node.
            emb_dim (int): Size of a vector in the embedding space.
            hidden_dim (int): Dimension of the hidden layers of the 
                ff-network layers within the graph-encoder.
            num_attention_layers (int): Number of attention layers 
                for both the graph-encoder and -decoder.
            num_heads (int): Number of attention heads in each 
                MultiHeadAttentionLayer for both the graph-encoder and -decoder.
        &#34;&#34;&#34;
        super().__init__()
        self.device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)

        self.encoder = GraphEncoder(
            node_input_dim=node_dim,
            embedding_dim=emb_dim,
            hidden_dim=hidden_dim,
            num_attention_layers=num_attention_layers,
            num_heads=num_heads,
        )
        self.decoder = GraphDecoder(
            emb_dim=emb_dim, num_heads=8, v_dim=emb_dim, k_dim=emb_dim
        )

        self.model = lambda x, mask, rollout: self.decoder(
            x, mask, rollout=rollout
        )  # remove encoding and make it do it once

    def forward(self, env, rollout=False) -&gt; Tuple[float, float]:
        &#34;&#34;&#34;
        Forward method of the model
        Args:
            env (gym.Env): environment which the agent has to solve.
            rollout (bool, optional): policy decision. Defaults to False.

        Returns:
            Tuple[float, float]: accumulated loss and log probabilities.
        &#34;&#34;&#34;
        done = False
        state = torch.tensor(env.get_state(), dtype=torch.float, device=self.device)
        acc_loss = torch.zeros(size=(state.shape[0],), device=self.device)
        acc_log_prob = torch.zeros(size=(state.shape[0],), device=self.device)

        emb = self.encoder(x=state[:, :, :2])

        while not done:
            actions, log_prob = self.decoder(
                node_embs=emb, mask=state[:, :, 3], rollout=rollout
            )

            state, loss, done, _ = env.step(actions.cpu().numpy())

            acc_loss += torch.tensor(loss, dtype=torch.float, device=self.device)
            acc_log_prob += log_prob.squeeze().to(self.device)

            state = torch.tensor(env.get_state(), dtype=torch.float, device=self.device)

        self.decoder.reset()

        return acc_loss, acc_log_prob  # shape (batch_size), shape (batch_size)


class TSPAgent:
    def __init__(
        self,
        node_dim: int = 2,
        emb_dim: int = 128,
        hidden_dim: int = 512,
        num_attention_layers: int = 3,
        num_heads: int = 8,
        lr: float = 1e-4,
        csv_path: str = &#34;loss_log.csv&#34;,
        seed=69,
    ):
        &#34;&#34;&#34;
        The TSPAgent is used in companionship with the TSPEnv
        to solve the traveling salesman problem.

        Args:
            node_dim (int): Input dimension of a regular graph node.
            emb_dim (int): Size of a vector in the embedding space.
            hidden_dim (int): Dimension of the hidden layers of the 
                ff-network layers within the graph-encoder.
            num_attention_layers (int): Number of attention layers 
                for both the graph-encoder and -decoder.
            num_heads (int): Number of attention heads in each 
                MultiHeadAttentionLayer for both the graph-encoder and -decoder.
            lr (float): learning rate.
            csv_path (string): file where the loss gets saved.
            seed (int): the seed.
        &#34;&#34;&#34;
        torch.manual_seed(seed)
        np.random.seed(seed)

        self.device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
        self.csv_path = csv_path
        self.model = TSPModel(
            node_dim=node_dim,
            emb_dim=emb_dim,
            hidden_dim=hidden_dim,
            num_attention_layers=num_attention_layers,
            num_heads=num_heads,
        ).to(self.device)

        self.target_model = TSPModel(
            node_dim=node_dim,
            emb_dim=emb_dim,
            hidden_dim=hidden_dim,
            num_attention_layers=num_attention_layers,
            num_heads=num_heads,
        ).to(self.device)

        self.target_model.load_state_dict(self.model.state_dict())
        self.target_model.eval()

        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)

    def train(
        self,
        env,
        epochs: int = 100,
        eval_epochs: int = 1,
        check_point_dir: str = &#34;./check_points/&#34;,
    ):
        &#34;&#34;&#34;
        Trains the TSPAgent on an TSPEnvironment.

        Args:
            env: TSPEnv instance to train on
            epochs (int, optional): Amount of epochs to train. Defaults to 100.
            eval_epochs (int, optional): Amount of epochs to evaluate the current 
                model against the baseline. Defaults to 1.
            check_point_dir (str, optional): Directiory that the checkpoints will
                be stored in. Defaults to &#34;./check_points/&#34;.
        &#34;&#34;&#34;
        logging.info(&#34;Start Training&#34;)
        with open(self.csv_path, &#34;w+&#34;, newline=&#34;&#34;) as file:
            writer = csv.writer(file)
            writer.writerow([&#34;Epoch&#34;, &#34;Loss&#34;, &#34;Cost&#34;, &#34;Advantage&#34;, &#34;Time&#34;])

        start_time = time.time()

        for e in range(epochs):
            self.model.train()

            loss_m, loss_b, log_prob = self.step(env, (False, True))
            advantage = (loss_m - loss_b) * -1
            loss = (advantage * log_prob).mean()

            # backpropagate
            self.opt.zero_grad()
            loss.backward()

            self.opt.step()

            # update model if better
            self.baseline_update(env, eval_epochs)

            logging.info(
                f&#34;Epoch {e} finished - Loss: {loss}, Advantage: {advantage.mean()} Dist: {loss_m.mean()}&#34;
            )

            # log training data
            with open(self.csv_path, &#34;a&#34;, newline=&#34;&#34;) as file:
                writer = csv.writer(file)
                writer.writerow(
                    [
                        e,
                        loss.item(),
                        loss_m.mean().item(),
                        advantage.mean().item(),
                        time.time() - start_time,
                    ]
                )

            self.save_model(episode=e, check_point_dir=check_point_dir)

    def save_model(self, episode: int, check_point_dir: str) -&gt; None:
        &#34;&#34;&#34;
        Saves the model parameters every 50 epochs.

        Args:
            episode (int): Current episode number
            check_point_dir (str): Directory where the checkpoints
                will be stored.
        &#34;&#34;&#34;
        if not os.path.exists(check_point_dir):
            os.makedirs(check_point_dir)

        if episode % 50 == 0 and episode != 0:
            torch.save(
                self.model.state_dict(), check_point_dir + f&#34;model_epoch_{episode}.pt&#34;,
            )

    def step(self, env, rollouts: Tuple[bool, bool]):
        &#34;&#34;&#34;
        Plays the environment to completion for
        both the baseline and the current model.

        Resets the environment beforehand.

        Args:
            env (gym.env): Environment to train on
            rollouts (Tuple[bool, bool]): Each entry decides 
                if we sample the actions from the learned
                distribution or act greedy. Indices are for
                the current model (0) and the baseline (1).

        Returns:
            (Tuple[torch.tensor, torch.tensor, torch.tensor]): 
                Tuple of the loss of the current model, the loss
                of the baseline and the log_probability for the 
                current model.
        &#34;&#34;&#34;
        env.reset()
        env_baseline = deepcopy(env)

        # Go through graph batch and get loss
        loss, log_prob = self.model(env, rollouts[0])
        with torch.no_grad():
            loss_b, _ = self.target_model(env_baseline, rollouts[0])

        return loss, loss_b, log_prob

    def evaluate(self, env):
        &#34;&#34;&#34;
        Evalutes the current model on the given environment.

        Args:
            env (gym.env): TSPAgent (or inherited) environment
                to evaluate
 
        Returns:
            torch.Tensor: Reward (e.g. -cost) of the current model.
        &#34;&#34;&#34;
        self.model.eval()

        with torch.no_grad():
            loss, _ = self.model(env, rollout=True)

        return loss

    def baseline_update(self, env, batch_steps: int = 3):
        &#34;&#34;&#34;
        Updates the baseline with the current model iff
        it perform significantly better than the baseline.

        Args:
            env (gym.env): Env to step through
            batch_steps (int, optional): How many games to play.
        &#34;&#34;&#34;
        logging.info(&#34;Update Baseline&#34;)
        self.model.eval()
        self.target_model.eval()

        current_model_cost = []
        baseline_model_cost = []
        with torch.no_grad():
            for _ in range(batch_steps):
                loss, loss_b, _ = self.step(env, [True, True])

                current_model_cost.append(loss)
                baseline_model_cost.append(loss_b)

        current_model_cost = torch.cat(current_model_cost)
        baseline_model_cost = torch.cat(baseline_model_cost)
        advantage = ((current_model_cost - baseline_model_cost) * -1).mean()
        _, p_value = stats.ttest_rel(
            current_model_cost.tolist(), baseline_model_cost.tolist()
        )

        if advantage.item() &lt;= 0 and p_value &lt;= 0.05:
            print(&#34;replacing baceline&#34;)
            self.target_model.load_state_dict(self.model.state_dict())</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="agents.graph_tsp_agent.TSPAgent"><code class="flex name class">
<span>class <span class="ident">TSPAgent</span></span>
<span>(</span><span>node_dim: int = 2, emb_dim: int = 128, hidden_dim: int = 512, num_attention_layers: int = 3, num_heads: int = 8, lr: float = 0.0001, csv_path: str = 'loss_log.csv', seed=69)</span>
</code></dt>
<dd>
<div class="desc"><p>The TSPAgent is used in companionship with the TSPEnv
to solve the traveling salesman problem.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>node_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Input dimension of a regular graph node.</dd>
<dt><strong><code>emb_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of a vector in the embedding space.</dd>
<dt><strong><code>hidden_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension of the hidden layers of the
ff-network layers within the graph-encoder.</dd>
<dt><strong><code>num_attention_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of attention layers
for both the graph-encoder and -decoder.</dd>
<dt><strong><code>num_heads</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of attention heads in each
MultiHeadAttentionLayer for both the graph-encoder and -decoder.</dd>
<dt><strong><code>lr</code></strong> :&ensp;<code>float</code></dt>
<dd>learning rate.</dd>
<dt><strong><code>csv_path</code></strong> :&ensp;<code>string</code></dt>
<dd>file where the loss gets saved.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>the seed.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TSPAgent:
    def __init__(
        self,
        node_dim: int = 2,
        emb_dim: int = 128,
        hidden_dim: int = 512,
        num_attention_layers: int = 3,
        num_heads: int = 8,
        lr: float = 1e-4,
        csv_path: str = &#34;loss_log.csv&#34;,
        seed=69,
    ):
        &#34;&#34;&#34;
        The TSPAgent is used in companionship with the TSPEnv
        to solve the traveling salesman problem.

        Args:
            node_dim (int): Input dimension of a regular graph node.
            emb_dim (int): Size of a vector in the embedding space.
            hidden_dim (int): Dimension of the hidden layers of the 
                ff-network layers within the graph-encoder.
            num_attention_layers (int): Number of attention layers 
                for both the graph-encoder and -decoder.
            num_heads (int): Number of attention heads in each 
                MultiHeadAttentionLayer for both the graph-encoder and -decoder.
            lr (float): learning rate.
            csv_path (string): file where the loss gets saved.
            seed (int): the seed.
        &#34;&#34;&#34;
        torch.manual_seed(seed)
        np.random.seed(seed)

        self.device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)
        self.csv_path = csv_path
        self.model = TSPModel(
            node_dim=node_dim,
            emb_dim=emb_dim,
            hidden_dim=hidden_dim,
            num_attention_layers=num_attention_layers,
            num_heads=num_heads,
        ).to(self.device)

        self.target_model = TSPModel(
            node_dim=node_dim,
            emb_dim=emb_dim,
            hidden_dim=hidden_dim,
            num_attention_layers=num_attention_layers,
            num_heads=num_heads,
        ).to(self.device)

        self.target_model.load_state_dict(self.model.state_dict())
        self.target_model.eval()

        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)

    def train(
        self,
        env,
        epochs: int = 100,
        eval_epochs: int = 1,
        check_point_dir: str = &#34;./check_points/&#34;,
    ):
        &#34;&#34;&#34;
        Trains the TSPAgent on an TSPEnvironment.

        Args:
            env: TSPEnv instance to train on
            epochs (int, optional): Amount of epochs to train. Defaults to 100.
            eval_epochs (int, optional): Amount of epochs to evaluate the current 
                model against the baseline. Defaults to 1.
            check_point_dir (str, optional): Directiory that the checkpoints will
                be stored in. Defaults to &#34;./check_points/&#34;.
        &#34;&#34;&#34;
        logging.info(&#34;Start Training&#34;)
        with open(self.csv_path, &#34;w+&#34;, newline=&#34;&#34;) as file:
            writer = csv.writer(file)
            writer.writerow([&#34;Epoch&#34;, &#34;Loss&#34;, &#34;Cost&#34;, &#34;Advantage&#34;, &#34;Time&#34;])

        start_time = time.time()

        for e in range(epochs):
            self.model.train()

            loss_m, loss_b, log_prob = self.step(env, (False, True))
            advantage = (loss_m - loss_b) * -1
            loss = (advantage * log_prob).mean()

            # backpropagate
            self.opt.zero_grad()
            loss.backward()

            self.opt.step()

            # update model if better
            self.baseline_update(env, eval_epochs)

            logging.info(
                f&#34;Epoch {e} finished - Loss: {loss}, Advantage: {advantage.mean()} Dist: {loss_m.mean()}&#34;
            )

            # log training data
            with open(self.csv_path, &#34;a&#34;, newline=&#34;&#34;) as file:
                writer = csv.writer(file)
                writer.writerow(
                    [
                        e,
                        loss.item(),
                        loss_m.mean().item(),
                        advantage.mean().item(),
                        time.time() - start_time,
                    ]
                )

            self.save_model(episode=e, check_point_dir=check_point_dir)

    def save_model(self, episode: int, check_point_dir: str) -&gt; None:
        &#34;&#34;&#34;
        Saves the model parameters every 50 epochs.

        Args:
            episode (int): Current episode number
            check_point_dir (str): Directory where the checkpoints
                will be stored.
        &#34;&#34;&#34;
        if not os.path.exists(check_point_dir):
            os.makedirs(check_point_dir)

        if episode % 50 == 0 and episode != 0:
            torch.save(
                self.model.state_dict(), check_point_dir + f&#34;model_epoch_{episode}.pt&#34;,
            )

    def step(self, env, rollouts: Tuple[bool, bool]):
        &#34;&#34;&#34;
        Plays the environment to completion for
        both the baseline and the current model.

        Resets the environment beforehand.

        Args:
            env (gym.env): Environment to train on
            rollouts (Tuple[bool, bool]): Each entry decides 
                if we sample the actions from the learned
                distribution or act greedy. Indices are for
                the current model (0) and the baseline (1).

        Returns:
            (Tuple[torch.tensor, torch.tensor, torch.tensor]): 
                Tuple of the loss of the current model, the loss
                of the baseline and the log_probability for the 
                current model.
        &#34;&#34;&#34;
        env.reset()
        env_baseline = deepcopy(env)

        # Go through graph batch and get loss
        loss, log_prob = self.model(env, rollouts[0])
        with torch.no_grad():
            loss_b, _ = self.target_model(env_baseline, rollouts[0])

        return loss, loss_b, log_prob

    def evaluate(self, env):
        &#34;&#34;&#34;
        Evalutes the current model on the given environment.

        Args:
            env (gym.env): TSPAgent (or inherited) environment
                to evaluate
 
        Returns:
            torch.Tensor: Reward (e.g. -cost) of the current model.
        &#34;&#34;&#34;
        self.model.eval()

        with torch.no_grad():
            loss, _ = self.model(env, rollout=True)

        return loss

    def baseline_update(self, env, batch_steps: int = 3):
        &#34;&#34;&#34;
        Updates the baseline with the current model iff
        it perform significantly better than the baseline.

        Args:
            env (gym.env): Env to step through
            batch_steps (int, optional): How many games to play.
        &#34;&#34;&#34;
        logging.info(&#34;Update Baseline&#34;)
        self.model.eval()
        self.target_model.eval()

        current_model_cost = []
        baseline_model_cost = []
        with torch.no_grad():
            for _ in range(batch_steps):
                loss, loss_b, _ = self.step(env, [True, True])

                current_model_cost.append(loss)
                baseline_model_cost.append(loss_b)

        current_model_cost = torch.cat(current_model_cost)
        baseline_model_cost = torch.cat(baseline_model_cost)
        advantage = ((current_model_cost - baseline_model_cost) * -1).mean()
        _, p_value = stats.ttest_rel(
            current_model_cost.tolist(), baseline_model_cost.tolist()
        )

        if advantage.item() &lt;= 0 and p_value &lt;= 0.05:
            print(&#34;replacing baceline&#34;)
            self.target_model.load_state_dict(self.model.state_dict())</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="agents.graph_irp_agent.IRPAgent" href="graph_irp_agent.html#agents.graph_irp_agent.IRPAgent">IRPAgent</a></li>
<li><a title="agents.graph_vrp_agent.VRPAgent" href="graph_vrp_agent.html#agents.graph_vrp_agent.VRPAgent">VRPAgent</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="agents.graph_tsp_agent.TSPAgent.baseline_update"><code class="name flex">
<span>def <span class="ident">baseline_update</span></span>(<span>self, env, batch_steps: int = 3)</span>
</code></dt>
<dd>
<div class="desc"><p>Updates the baseline with the current model iff
it perform significantly better than the baseline.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>gym.env</code></dt>
<dd>Env to step through</dd>
<dt><strong><code>batch_steps</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>How many games to play.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def baseline_update(self, env, batch_steps: int = 3):
    &#34;&#34;&#34;
    Updates the baseline with the current model iff
    it perform significantly better than the baseline.

    Args:
        env (gym.env): Env to step through
        batch_steps (int, optional): How many games to play.
    &#34;&#34;&#34;
    logging.info(&#34;Update Baseline&#34;)
    self.model.eval()
    self.target_model.eval()

    current_model_cost = []
    baseline_model_cost = []
    with torch.no_grad():
        for _ in range(batch_steps):
            loss, loss_b, _ = self.step(env, [True, True])

            current_model_cost.append(loss)
            baseline_model_cost.append(loss_b)

    current_model_cost = torch.cat(current_model_cost)
    baseline_model_cost = torch.cat(baseline_model_cost)
    advantage = ((current_model_cost - baseline_model_cost) * -1).mean()
    _, p_value = stats.ttest_rel(
        current_model_cost.tolist(), baseline_model_cost.tolist()
    )

    if advantage.item() &lt;= 0 and p_value &lt;= 0.05:
        print(&#34;replacing baceline&#34;)
        self.target_model.load_state_dict(self.model.state_dict())</code></pre>
</details>
</dd>
<dt id="agents.graph_tsp_agent.TSPAgent.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, env)</span>
</code></dt>
<dd>
<div class="desc"><p>Evalutes the current model on the given environment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>gym.env</code></dt>
<dd>TSPAgent (or inherited) environment
to evaluate</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Reward (e.g. -cost) of the current model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, env):
    &#34;&#34;&#34;
    Evalutes the current model on the given environment.

    Args:
        env (gym.env): TSPAgent (or inherited) environment
            to evaluate

    Returns:
        torch.Tensor: Reward (e.g. -cost) of the current model.
    &#34;&#34;&#34;
    self.model.eval()

    with torch.no_grad():
        loss, _ = self.model(env, rollout=True)

    return loss</code></pre>
</details>
</dd>
<dt id="agents.graph_tsp_agent.TSPAgent.save_model"><code class="name flex">
<span>def <span class="ident">save_model</span></span>(<span>self, episode: int, check_point_dir: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the model parameters every 50 epochs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>episode</code></strong> :&ensp;<code>int</code></dt>
<dd>Current episode number</dd>
<dt><strong><code>check_point_dir</code></strong> :&ensp;<code>str</code></dt>
<dd>Directory where the checkpoints
will be stored.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_model(self, episode: int, check_point_dir: str) -&gt; None:
    &#34;&#34;&#34;
    Saves the model parameters every 50 epochs.

    Args:
        episode (int): Current episode number
        check_point_dir (str): Directory where the checkpoints
            will be stored.
    &#34;&#34;&#34;
    if not os.path.exists(check_point_dir):
        os.makedirs(check_point_dir)

    if episode % 50 == 0 and episode != 0:
        torch.save(
            self.model.state_dict(), check_point_dir + f&#34;model_epoch_{episode}.pt&#34;,
        )</code></pre>
</details>
</dd>
<dt id="agents.graph_tsp_agent.TSPAgent.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, env, rollouts: Tuple[bool, bool])</span>
</code></dt>
<dd>
<div class="desc"><p>Plays the environment to completion for
both the baseline and the current model.</p>
<p>Resets the environment beforehand.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>gym.env</code></dt>
<dd>Environment to train on</dd>
<dt><strong><code>rollouts</code></strong> :&ensp;<code>Tuple[bool, bool]</code></dt>
<dd>Each entry decides
if we sample the actions from the learned
distribution or act greedy. Indices are for
the current model (0) and the baseline (1).</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>(Tuple[torch.tensor, torch.tensor, torch.tensor]):
Tuple of the loss of the current model, the loss
of the baseline and the log_probability for the
current model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, env, rollouts: Tuple[bool, bool]):
    &#34;&#34;&#34;
    Plays the environment to completion for
    both the baseline and the current model.

    Resets the environment beforehand.

    Args:
        env (gym.env): Environment to train on
        rollouts (Tuple[bool, bool]): Each entry decides 
            if we sample the actions from the learned
            distribution or act greedy. Indices are for
            the current model (0) and the baseline (1).

    Returns:
        (Tuple[torch.tensor, torch.tensor, torch.tensor]): 
            Tuple of the loss of the current model, the loss
            of the baseline and the log_probability for the 
            current model.
    &#34;&#34;&#34;
    env.reset()
    env_baseline = deepcopy(env)

    # Go through graph batch and get loss
    loss, log_prob = self.model(env, rollouts[0])
    with torch.no_grad():
        loss_b, _ = self.target_model(env_baseline, rollouts[0])

    return loss, loss_b, log_prob</code></pre>
</details>
</dd>
<dt id="agents.graph_tsp_agent.TSPAgent.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, env, epochs: int = 100, eval_epochs: int = 1, check_point_dir: str = './check_points/')</span>
</code></dt>
<dd>
<div class="desc"><p>Trains the TSPAgent on an TSPEnvironment.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>env</code></strong></dt>
<dd>TSPEnv instance to train on</dd>
<dt><strong><code>epochs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Amount of epochs to train. Defaults to 100.</dd>
<dt><strong><code>eval_epochs</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Amount of epochs to evaluate the current
model against the baseline. Defaults to 1.</dd>
<dt><strong><code>check_point_dir</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Directiory that the checkpoints will
be stored in. Defaults to "./check_points/".</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(
    self,
    env,
    epochs: int = 100,
    eval_epochs: int = 1,
    check_point_dir: str = &#34;./check_points/&#34;,
):
    &#34;&#34;&#34;
    Trains the TSPAgent on an TSPEnvironment.

    Args:
        env: TSPEnv instance to train on
        epochs (int, optional): Amount of epochs to train. Defaults to 100.
        eval_epochs (int, optional): Amount of epochs to evaluate the current 
            model against the baseline. Defaults to 1.
        check_point_dir (str, optional): Directiory that the checkpoints will
            be stored in. Defaults to &#34;./check_points/&#34;.
    &#34;&#34;&#34;
    logging.info(&#34;Start Training&#34;)
    with open(self.csv_path, &#34;w+&#34;, newline=&#34;&#34;) as file:
        writer = csv.writer(file)
        writer.writerow([&#34;Epoch&#34;, &#34;Loss&#34;, &#34;Cost&#34;, &#34;Advantage&#34;, &#34;Time&#34;])

    start_time = time.time()

    for e in range(epochs):
        self.model.train()

        loss_m, loss_b, log_prob = self.step(env, (False, True))
        advantage = (loss_m - loss_b) * -1
        loss = (advantage * log_prob).mean()

        # backpropagate
        self.opt.zero_grad()
        loss.backward()

        self.opt.step()

        # update model if better
        self.baseline_update(env, eval_epochs)

        logging.info(
            f&#34;Epoch {e} finished - Loss: {loss}, Advantage: {advantage.mean()} Dist: {loss_m.mean()}&#34;
        )

        # log training data
        with open(self.csv_path, &#34;a&#34;, newline=&#34;&#34;) as file:
            writer = csv.writer(file)
            writer.writerow(
                [
                    e,
                    loss.item(),
                    loss_m.mean().item(),
                    advantage.mean().item(),
                    time.time() - start_time,
                ]
            )

        self.save_model(episode=e, check_point_dir=check_point_dir)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="agents.graph_tsp_agent.TSPModel"><code class="flex name class">
<span>class <span class="ident">TSPModel</span></span>
<span>(</span><span>node_dim: int, emb_dim: int, hidden_dim: int, num_attention_layers: int, num_heads: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>The TSPModel is used in companionship with the TSPEnv
to solve the capacited vehicle routing problem.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>depot_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Input dimension of a depot node.</dd>
<dt><strong><code>node_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Input dimension of a regular graph node.</dd>
<dt><strong><code>emb_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of a vector in the embedding space.</dd>
<dt><strong><code>hidden_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension of the hidden layers of the
ff-network layers within the graph-encoder.</dd>
<dt><strong><code>num_attention_layers</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of attention layers
for both the graph-encoder and -decoder.</dd>
<dt><strong><code>num_heads</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of attention heads in each
MultiHeadAttentionLayer for both the graph-encoder and -decoder.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TSPModel(nn.Module):
    def __init__(
        self,
        node_dim: int,
        emb_dim: int,
        hidden_dim: int,
        num_attention_layers: int,
        num_heads: int,
    ):
        &#34;&#34;&#34;
        The TSPModel is used in companionship with the TSPEnv
        to solve the capacited vehicle routing problem.

        Args:
            depot_dim (int): Input dimension of a depot node.
            node_dim (int): Input dimension of a regular graph node.
            emb_dim (int): Size of a vector in the embedding space.
            hidden_dim (int): Dimension of the hidden layers of the 
                ff-network layers within the graph-encoder.
            num_attention_layers (int): Number of attention layers 
                for both the graph-encoder and -decoder.
            num_heads (int): Number of attention heads in each 
                MultiHeadAttentionLayer for both the graph-encoder and -decoder.
        &#34;&#34;&#34;
        super().__init__()
        self.device = torch.device(&#34;cuda:0&#34; if torch.cuda.is_available() else &#34;cpu&#34;)

        self.encoder = GraphEncoder(
            node_input_dim=node_dim,
            embedding_dim=emb_dim,
            hidden_dim=hidden_dim,
            num_attention_layers=num_attention_layers,
            num_heads=num_heads,
        )
        self.decoder = GraphDecoder(
            emb_dim=emb_dim, num_heads=8, v_dim=emb_dim, k_dim=emb_dim
        )

        self.model = lambda x, mask, rollout: self.decoder(
            x, mask, rollout=rollout
        )  # remove encoding and make it do it once

    def forward(self, env, rollout=False) -&gt; Tuple[float, float]:
        &#34;&#34;&#34;
        Forward method of the model
        Args:
            env (gym.Env): environment which the agent has to solve.
            rollout (bool, optional): policy decision. Defaults to False.

        Returns:
            Tuple[float, float]: accumulated loss and log probabilities.
        &#34;&#34;&#34;
        done = False
        state = torch.tensor(env.get_state(), dtype=torch.float, device=self.device)
        acc_loss = torch.zeros(size=(state.shape[0],), device=self.device)
        acc_log_prob = torch.zeros(size=(state.shape[0],), device=self.device)

        emb = self.encoder(x=state[:, :, :2])

        while not done:
            actions, log_prob = self.decoder(
                node_embs=emb, mask=state[:, :, 3], rollout=rollout
            )

            state, loss, done, _ = env.step(actions.cpu().numpy())

            acc_loss += torch.tensor(loss, dtype=torch.float, device=self.device)
            acc_log_prob += log_prob.squeeze().to(self.device)

            state = torch.tensor(env.get_state(), dtype=torch.float, device=self.device)

        self.decoder.reset()

        return acc_loss, acc_log_prob  # shape (batch_size), shape (batch_size)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="agents.graph_irp_agent.IRPModel" href="graph_irp_agent.html#agents.graph_irp_agent.IRPModel">IRPModel</a></li>
<li><a title="agents.graph_vrp_agent.VRPModel" href="graph_vrp_agent.html#agents.graph_vrp_agent.VRPModel">VRPModel</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="agents.graph_tsp_agent.TSPModel.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="agents.graph_tsp_agent.TSPModel.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="agents.graph_tsp_agent.TSPModel.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, env, rollout=False) ‑> Tuple[float, float]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward method of the model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>env</code></strong> :&ensp;<code>gym.Env</code></dt>
<dd>environment which the agent has to solve.</dd>
<dt><strong><code>rollout</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>policy decision. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple[float, float]</code></dt>
<dd>accumulated loss and log probabilities.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, env, rollout=False) -&gt; Tuple[float, float]:
    &#34;&#34;&#34;
    Forward method of the model
    Args:
        env (gym.Env): environment which the agent has to solve.
        rollout (bool, optional): policy decision. Defaults to False.

    Returns:
        Tuple[float, float]: accumulated loss and log probabilities.
    &#34;&#34;&#34;
    done = False
    state = torch.tensor(env.get_state(), dtype=torch.float, device=self.device)
    acc_loss = torch.zeros(size=(state.shape[0],), device=self.device)
    acc_log_prob = torch.zeros(size=(state.shape[0],), device=self.device)

    emb = self.encoder(x=state[:, :, :2])

    while not done:
        actions, log_prob = self.decoder(
            node_embs=emb, mask=state[:, :, 3], rollout=rollout
        )

        state, loss, done, _ = env.step(actions.cpu().numpy())

        acc_loss += torch.tensor(loss, dtype=torch.float, device=self.device)
        acc_log_prob += log_prob.squeeze().to(self.device)

        state = torch.tensor(env.get_state(), dtype=torch.float, device=self.device)

    self.decoder.reset()

    return acc_loss, acc_log_prob  # shape (batch_size), shape (batch_size)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="agents" href="index.html">agents</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="agents.graph_tsp_agent.TSPAgent" href="#agents.graph_tsp_agent.TSPAgent">TSPAgent</a></code></h4>
<ul class="">
<li><code><a title="agents.graph_tsp_agent.TSPAgent.baseline_update" href="#agents.graph_tsp_agent.TSPAgent.baseline_update">baseline_update</a></code></li>
<li><code><a title="agents.graph_tsp_agent.TSPAgent.evaluate" href="#agents.graph_tsp_agent.TSPAgent.evaluate">evaluate</a></code></li>
<li><code><a title="agents.graph_tsp_agent.TSPAgent.save_model" href="#agents.graph_tsp_agent.TSPAgent.save_model">save_model</a></code></li>
<li><code><a title="agents.graph_tsp_agent.TSPAgent.step" href="#agents.graph_tsp_agent.TSPAgent.step">step</a></code></li>
<li><code><a title="agents.graph_tsp_agent.TSPAgent.train" href="#agents.graph_tsp_agent.TSPAgent.train">train</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="agents.graph_tsp_agent.TSPModel" href="#agents.graph_tsp_agent.TSPModel">TSPModel</a></code></h4>
<ul class="">
<li><code><a title="agents.graph_tsp_agent.TSPModel.dump_patches" href="#agents.graph_tsp_agent.TSPModel.dump_patches">dump_patches</a></code></li>
<li><code><a title="agents.graph_tsp_agent.TSPModel.forward" href="#agents.graph_tsp_agent.TSPModel.forward">forward</a></code></li>
<li><code><a title="agents.graph_tsp_agent.TSPModel.training" href="#agents.graph_tsp_agent.TSPModel.training">training</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>